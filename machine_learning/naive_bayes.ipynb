{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (2.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (2.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: packaging in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: namex in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: rich in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: joblib in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install tensorflow\n",
    "%pip install pandas\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/divyendu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Data Loading and Preprocessing\n",
    "\n",
    "def load_vocabulary(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        vocabulary = [line.strip() for line in f]\n",
    "    return vocabulary\n",
    "\n",
    "def load_newsgrouplabels(file_path):\n",
    "    newsgrouplabels = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            newsgrouplabels.append(parts[0]) # Take only the newsgroup name (first part)\n",
    "    return newsgrouplabels\n",
    "\n",
    "def load_labels(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        labels = [int(line.strip()) for line in f]\n",
    "    return labels\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            doc_id, word_id, count = map(int, line.strip().split())\n",
    "            data.append({'docId': doc_id, 'wordId': word_id, 'count': count})\n",
    "    return data\n",
    "\n",
    "def process_vocabulary(vocabulary):\n",
    "    processed_vocabulary = []\n",
    "    punctuation_remover = str.maketrans('', '', string.punctuation)\n",
    "    stop_words = ENGLISH_STOP_WORDS\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for word in vocabulary:\n",
    "        word_lower = word.lower()\n",
    "        word_no_punctuation = word_lower.translate(punctuation_remover)\n",
    "        # if word_no_punctuation and word_no_punctuation not in stop_words:\n",
    "        word_stemmed = stemmer.stem(word_no_punctuation)\n",
    "        word_lemmatized = lemmatizer.lemmatize(word_stemmed)\n",
    "        processed_vocabulary.append(word_lemmatized)\n",
    "\n",
    "    return processed_vocabulary\n",
    "\n",
    "# Load data\n",
    "vocabulary = process_vocabulary(load_vocabulary('vocabulary.txt'))\n",
    "newsgrouplabels = load_newsgrouplabels('train.map')\n",
    "train_labels = load_labels('train.label')\n",
    "test_labels = load_labels('test.label')\n",
    "train_data = load_data('train.data')\n",
    "test_data = load_data('test.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<11269x39128 sparse matrix of type '<class 'numpy.float64'>'\n",
      " \twith 1397826 stored elements in Compressed Sparse Row format>]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[<7505x39128 sparse matrix of type '<class 'numpy.float64'>'\n",
      " \twith 910707 stored elements in Compressed Sparse Row format>]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    }
   ],
   "source": [
    "# Convert data to document-term matrix format\n",
    "def create_document_term_matrix(data, data_labels, vocab_size, vectorizer=None):\n",
    "    doc_term_lists = [[] for _ in range(len(data_labels))]\n",
    "    for item in data:\n",
    "        doc_id = item['docId']\n",
    "        word_id = item['wordId']\n",
    "        count = item['count']\n",
    "        doc_term_lists[doc_id - 1].append({'word_id': word_id, 'count': count})\n",
    "\n",
    "    documents = []\n",
    "    for doc_terms in doc_term_lists:\n",
    "        doc_dict = {}\n",
    "        for term_info in doc_terms:\n",
    "            doc_dict[vocabulary[term_info['word_id'] - 1]] = doc_dict.get(vocabulary[term_info['word_id'] - 1], 0) + term_info['count']\n",
    "        documents.append(doc_dict)\n",
    "\n",
    "    if vectorizer is None:\n",
    "        vectorizer = DictVectorizer()\n",
    "        X = vectorizer.fit_transform(documents)\n",
    "        feature_names = vectorizer.feature_names_\n",
    "        return X, np.array(data_labels), feature_names, vectorizer\n",
    "    else:\n",
    "        X = vectorizer.transform(documents)\n",
    "        # Return vectorizer even if not fitted in this call for consistency\n",
    "        return X, np.array(data_labels), vectorizer.feature_names_ , vectorizer\n",
    "\n",
    "\n",
    "# Create document-term matrices for train and test data\n",
    "vocab_size = len(vocabulary)\n",
    "X_train_counts, y_train, feature_names, vectorizer = create_document_term_matrix(train_data, train_labels, vocab_size)\n",
    "# Pass fitted vectorizer\n",
    "X_test_counts, y_test, _, _ = create_document_term_matrix(test_data, test_labels, vocab_size, vectorizer=vectorizer)\n",
    "\n",
    "print(np.unique(X_train_counts))\n",
    "print(np.unique(y_train))\n",
    "print(np.unique(X_test_counts))\n",
    "print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Train and Evaluate Naive Bayes and Logistic Regression\n",
    "\n",
    "# Naive Bayes\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train_counts, y_train)\n",
    "y_pred_nb = model_nb.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model_lr = LogisticRegression(max_iter=1000)\n",
    "model_lr.fit(X_train_counts, y_train)\n",
    "y_pred_lr = model_lr.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[    0     1     2 ... 53973 53974 53975]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[    0     1     2 ... 61184 61187 61188]\n"
     ]
    }
   ],
   "source": [
    "# Part 3: CNN Classifier\n",
    "# Prepare data for CNN - using word IDs directly as input sequences per document\n",
    "\n",
    "def create_document_word_id_sequences(data, data_labels, max_sequence_length):\n",
    "    doc_word_id_lists = [[] for _ in range(len(data_labels))]\n",
    "    for item in data:\n",
    "        doc_id = item['docId']\n",
    "        word_id = item['wordId']\n",
    "        count = item['count']\n",
    "        for _ in range(count):\n",
    "            doc_word_id_lists[doc_id - 1].append(word_id)\n",
    "\n",
    "    X_cnn = []\n",
    "    for word_ids in doc_word_id_lists:\n",
    "        # Truncate sequences longer than max_sequence_length\n",
    "        if len(word_ids) > max_sequence_length:\n",
    "            word_ids = word_ids[:max_sequence_length]\n",
    "        # Pad sequences shorter than max_sequence_length\n",
    "        else:\n",
    "            padding_needed = max_sequence_length - len(word_ids)\n",
    "            word_ids.extend([0] * padding_needed) # Use 0 as padding value\n",
    "        X_cnn.append(word_ids)\n",
    "\n",
    "    y_cnn = np.array([i - 1 for i in data_labels])\n",
    "    return np.array(X_cnn), y_cnn\n",
    "\n",
    "max_sequence_length = 500 # Adjust as needed, based on document lengths. Can calculate from train data.\n",
    "X_train_cnn, y_train_cnn = create_document_word_id_sequences(train_data, train_labels, max_sequence_length)\n",
    "X_test_cnn, y_test_cnn = create_document_word_id_sequences(test_data, test_labels, max_sequence_length)\n",
    "\n",
    "num_classes = len(newsgrouplabels)\n",
    " # adding 1 for padded zeros. So if there are n words, and we pad with zero, we will have n + 1 values.\n",
    "vocab_size_cnn = len(vocabulary) + 1 \n",
    "\n",
    "print(np.unique(y_train_cnn))\n",
    "print(np.unique(X_train_cnn))\n",
    "print(np.unique(y_test_cnn))\n",
    "print(np.unique(X_test_cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "embedding_dim = 100\n",
    "num_filters = 128\n",
    "kernel_size = 5\n",
    "pool_size = 4\n",
    "\n",
    "model_cnn = Sequential([\n",
    "    Embedding(input_dim=vocab_size_cnn, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu'),\n",
    "    MaxPooling1D(pool_size=pool_size),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train CNN\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model_cnn.fit(X_train_cnn, y_train_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1, verbose=0) # Silent training\n",
    "\n",
    "# Evaluate CNN\n",
    "y_pred_cnn_probs = model_cnn.predict(X_test_cnn, verbose=0) # Silent prediction\n",
    "y_pred_cnn = np.argmax(y_pred_cnn_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Naive Bayes:\n",
      "\n",
      "Overall Accuracy: 0.7847\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "\n",
      "Overall Accuracy: 0.7320\n",
      "\n",
      "Classification Report for CNN:\n",
      "\n",
      "Overall Accuracy: 0.7078\n",
      "\n",
      "Naive Bayes Results:\n",
      "    class_id  Precision    Recall  F1-score  support\n",
      "0          1   0.704348  0.764151  0.733032      318\n",
      "1          2   0.654267  0.768638  0.706856      389\n",
      "2          3   0.834711  0.516624  0.638231      391\n",
      "3          4   0.604040  0.762755  0.674183      392\n",
      "4          5   0.736434  0.744125  0.740260      383\n",
      "5          6   0.822222  0.758974  0.789333      390\n",
      "6          7   0.913043  0.604712  0.727559      382\n",
      "7          8   0.800895  0.906329  0.850356      395\n",
      "8          9   0.902256  0.906801  0.904523      397\n",
      "9         10   0.954178  0.891688  0.921875      397\n",
      "10        11   0.950249  0.957393  0.953808      399\n",
      "11        12   0.787281  0.908861  0.843713      395\n",
      "12        13   0.769452  0.679389  0.721622      393\n",
      "13        14   0.892958  0.806616  0.847594      393\n",
      "14        15   0.875325  0.859694  0.867439      392\n",
      "15        16   0.715370  0.947236  0.815135      398\n",
      "16        17   0.667351  0.892857  0.763807      364\n",
      "17        18   0.899160  0.853723  0.875853      376\n",
      "18        19   0.578275  0.583871  0.581059      310\n",
      "19        20   0.800000  0.382470  0.517520      251\n",
      "\n",
      "Logistic Regression Results:\n",
      "    class_id  Precision    Recall  F1-score  support\n",
      "0          1   0.664537  0.654088  0.659271      318\n",
      "1          2   0.617512  0.688946  0.651276      389\n",
      "2          3   0.673410  0.595908  0.632293      391\n",
      "3          4   0.626794  0.668367  0.646914      392\n",
      "4          5   0.678487  0.749347  0.712159      383\n",
      "5          6   0.761062  0.661538  0.707819      390\n",
      "6          7   0.710112  0.827225  0.764208      382\n",
      "7          8   0.759709  0.792405  0.775713      395\n",
      "8          9   0.867500  0.874055  0.870765      397\n",
      "9         10   0.805226  0.853904  0.828851      397\n",
      "10        11   0.908163  0.892231  0.900126      399\n",
      "11        12   0.831551  0.787342  0.808843      395\n",
      "12        13   0.614610  0.620865  0.617722      393\n",
      "13        14   0.790634  0.730280  0.759259      393\n",
      "14        15   0.855586  0.801020  0.827404      392\n",
      "15        16   0.766990  0.793970  0.780247      398\n",
      "16        17   0.654930  0.766484  0.706329      364\n",
      "17        18   0.886076  0.744681  0.809249      376\n",
      "18        19   0.600791  0.490323  0.539964      310\n",
      "19        20   0.488189  0.494024  0.491089      251\n",
      "\n",
      "CNN Results:\n",
      "    class_id  Precision    Recall  F1-score  support\n",
      "0          0   0.558747  0.672956  0.610556      318\n",
      "1          1   0.571698  0.778920  0.659412      389\n",
      "2          2   0.735465  0.647059  0.688435      391\n",
      "3          3   0.614379  0.719388  0.662750      392\n",
      "4          4   0.753281  0.749347  0.751309      383\n",
      "5          5   0.788162  0.648718  0.711674      390\n",
      "6          6   0.785714  0.748691  0.766756      382\n",
      "7          7   0.794189  0.830380  0.811881      395\n",
      "8          8   0.884712  0.889169  0.886935      397\n",
      "9          9   0.865079  0.823678  0.843871      397\n",
      "10        10   0.861502  0.919799  0.889697      399\n",
      "11        11   0.800959  0.845570  0.822660      395\n",
      "12        12   0.607059  0.656489  0.630807      393\n",
      "13        13   0.697619  0.745547  0.720787      393\n",
      "14        14   0.759434  0.821429  0.789216      392\n",
      "15        15   0.570033  0.879397  0.691700      398\n",
      "16        16   0.522282  0.804945  0.633514      364\n",
      "17        17   0.849593  0.555851  0.672026      376\n",
      "18        18   0.000000  0.000000  0.000000      310\n",
      "19        19   0.000000  0.000000  0.000000      251\n",
      "\n",
      "Overall Accuracies:\n",
      "Naive Bayes Accuracy: 0.7847\n",
      "Logistic Regression Accuracy: 0.7320\n",
      "CNN Accuracy: 0.7078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyendu/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/divyendu/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/divyendu/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Part 4: Evaluation and Report\n",
    "\n",
    "def evaluate_classifier(y_true, y_pred, labels, classifier_name):\n",
    "    target_names_list = [labels[i - 1] for i in np.unique(y_true)]\n",
    "    report = classification_report(y_true, y_pred, labels=np.unique(y_true), target_names=target_names_list, output_dict=True)\n",
    "    print(f\"Classification Report for {classifier_name}:\\n\")\n",
    "    table_data = []\n",
    "    for class_id in np.unique(y_true):\n",
    "        class_label = labels[class_id - 1]\n",
    "        row = {\n",
    "            'class_id': class_id,\n",
    "            'Precision': report[class_label]['precision'],\n",
    "            'Recall': report[class_label]['recall'],\n",
    "            'F1-score': report[class_label]['f1-score'],\n",
    "            'support': int(report[class_label]['support'])\n",
    "        }\n",
    "        table_data.append(row)\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = report['accuracy']\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\\n\")\n",
    "\n",
    "    return table_data, overall_accuracy\n",
    "\n",
    "\n",
    "# Evaluate and print reports\n",
    "nb_table, nb_accuracy = evaluate_classifier(y_test, y_pred_nb, newsgrouplabels, \"Naive Bayes\")\n",
    "lr_table, lr_accuracy = evaluate_classifier(y_test, y_pred_lr, newsgrouplabels, \"Logistic Regression\")\n",
    "cnn_table, cnn_accuracy = evaluate_classifier(y_test_cnn, y_pred_cnn, newsgrouplabels, \"CNN\")\n",
    "\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(pd.DataFrame(nb_table))\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(pd.DataFrame(lr_table))\n",
    "print(\"\\nCNN Results:\")\n",
    "print(pd.DataFrame(cnn_table))\n",
    "\n",
    "\n",
    "print(f\"\\nOverall Accuracies:\")\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"CNN Accuracy: {cnn_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
