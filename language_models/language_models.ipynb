{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6120 35650: Spring2025 Assignment 5\n",
    "Instructor: Amir Tahmasebi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5 - Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models: Auto-Complete\n",
    "\n",
    "In this assignment, you will build an auto-complete system.\n",
    "\n",
    "A key building block for an auto-complete system is language models.\n",
    "A Language models assigns the probability to a sequence of words, in a way that more \"likely\" sequences receives higher scores.  \n",
    "\n",
    "While a variety of language models have been developed, this assignment uses **N-grams**, a simple but still powerful methodology for language modeling.\n",
    "N-grams are used not only for auto-complete systems, but are also useful in machine translation, speech recognition, and could even help you classify authors given a text. \n",
    "\n",
    "\n",
    "Here is the steps of this assignment:\n",
    "\n",
    "1. Load and preprocess data\n",
    "    - Load and tokenize data.\n",
    "    - Split the sentences into training and test sets.\n",
    "    - Replace words with a low frequency by an unknown marker '<unk>'.\n",
    "1. Develop N-gram based language models\n",
    "    - Compute the count of n-grams from a given data set.\n",
    "    - Estimate the conditional probability of a next word with k-smoothing.\n",
    "1. Evaluate the N-gram models by computing the perplexity score.\n",
    "1. Use your own model to suggest an upcoming word given your sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/divyendu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('.')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and Preprocess Data\n",
    "\n",
    "We will be using twitter data for our exercise.\n",
    "Let's load the data and see the first few sentences by the running the next cell.\n",
    "\n",
    "Notice that data is a long string that contains many many tweats.\n",
    "Observe that we have a linebreak \"\\n\" between two sentences (tweats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 164456396\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'od. I see the success you got poppin in yo area.\\nRT : Consumers are visual. They want data at their finger tips. Mobile is the only way to deliver this, 24/7.\\nu welcome\\nIt is #RHONJ time!!\\nThe key to keeping your woman happy= attention, affection, treat her like a queen and sex her like a pornstar!\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open(\"./en_US.twitter.txt\", \"r\") as f: # update the path accordingly.\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess this data by the following steps:\n",
    "\n",
    "1. Split data into sentences.\n",
    "1. Split each sentence into tokens.\n",
    "1. Split sentences into training and test data.\n",
    "1. Find tokens that appear at least N times in the training data.\n",
    "1. Replace tokens that appear less than N times by \"<unk\\>\"\n",
    "\n",
    "Note that in this assignment we use \"token\" and \"words\" interchangeably.\n",
    "\n",
    "Note: we omit validation data in this exercise.\n",
    "In real applications, we should hold a part of data as a validation set and use it to tune our training.\n",
    "We skip this process for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the data processing is to split this large string into sentences.  For simplicity, we will use the linebreak \"\\n\" as the delimiter.\n",
    "\n",
    "Hint: Use [str.split](https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed: split_to_sentences ###\n",
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    \n",
    "    Args:\n",
    "        data: str\n",
    "    \n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    sentences = data.strip().split(\"\\n\")\n",
    "    \n",
    "    return sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I have a pen.\n",
      "I have an apple. \n",
      "Ah\n",
      "Apple pen.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I have a pen.', 'I have an apple. ', 'Ah', 'Apple pen.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "x = \"\"\"\n",
    "I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n",
    "\"\"\"\n",
    "print(x)\n",
    "\n",
    "split_to_sentences(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected answer: \n",
    "```\n",
    "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenize sentences, that is, we split a sentence into a sequence of tokens (words). \n",
    "We also convert all tokens into lower cases not to distinguish the words appearing at the beginning.\n",
    "\n",
    "Hint:\n",
    "Use [str.lower](https://docs.python.org/3/library/stdtypes.html?highlight=split#str.lower) method for converting strings to lower cases.\n",
    "For splitting a sentence into tokens, [nltk.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize) function is helpful.(Implementation with [str.split](https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split) would require some additional edge cases (e.g. comma or period right after a word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed: tokenize_sentences ###\n",
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into tokens (words)\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentences.append(nltk.word_tokenize(sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sky', 'is', 'blue', '.'],\n",
       " ['Leaves', 'are', 'green', '.'],\n",
       " ['Roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
    "tokenize_sentences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected answer:\n",
    "\n",
    "```\n",
    "[['sky', 'is', 'blue', '.'],\n",
    " ['leaves', 'are', 'green', '.'],\n",
    " ['roses', 'are', 'red', '.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example for the previous function:\n",
    "def test_tokenize_sentences():\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"notebook example\",\n",
    "            \"input\": [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"],\n",
    "            \"expected\": [['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green', '.'], ['roses', 'are', 'red', '.']]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"no sentence\",\n",
    "            \"input\": [],\n",
    "            \"expected\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"one sentence with ;\",\n",
    "            \"input\": [\"Grass is greener;\"],\n",
    "            \"expected\": [['grass', 'is', 'greener', ';']]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"two sentences, one in CAPS\",\n",
    "            \"input\": [\"Space if infinite.\", \"OR IS IT?\"],\n",
    "            \"expected\": [['space', 'if', 'infinite', '.'], ['or', 'is', 'it', '?']]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"long sentence\",\n",
    "            \"input\": [\"Really really long sentence. It is very long indeed; so long...\"],\n",
    "            \"expected\": [['really', 'really', 'long', 'sentence', '.', 'it', 'is', 'very', 'long', 'indeed', ';', 'so', 'long', '...']]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        assert test_case[\"expected\"] == tokenize_sentences(test_case[\"input\"])\n",
    "        print('Successfully passed test case: ' + test_case[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As covered in the lecture, we won't be using all the tokens (words) appearing in the data for training.  Instead, we will use only the words that appear at least N times in the data.\n",
    "To do so, let's count up all the words in the data set.\n",
    "\n",
    "Hint: You need a double for-loop, one for sentences and the other for tokens within a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function above, we can now preprare the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed: get_tokenized_data ###\n",
    "def get_tokenized_data(data):\n",
    "    \"\"\"\n",
    "    Make a list of tokenized sentences\n",
    "    \n",
    "    Args:\n",
    "        data: String\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = split_to_sentences(data)\n",
    "\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sky', 'is', 'blue', '.'],\n",
       " ['Leaves', 'are', 'green'],\n",
       " ['Roses', 'are', 'red', '.']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\n",
    "get_tokenized_data(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected outcome:\n",
    "\n",
    "```\n",
    "[['sky', 'is', 'blue', '.'],\n",
    " ['leaves', 'are', 'green'],\n",
    " ['roses', 'are', 'red', '.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cell below to split data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360148 data are split into 1888118 train and 472030 test set\n",
      "First training sample:\n",
      "['Adam', 'Sandler', 'is', 'not', 'being', 'funny', 'on', '#', 'peoplechoice', '.', 'Make', 'it', 'stop']\n",
      "First test sample\n",
      "['“', ':', 'Dwayne', 'Wade', 'accused', 'Pacers', 'of', 'excessive', 'celebration', '.', 'the', 'Heat', 'would', 'NEVER', 'celebrate', 'before', 'they', 'won', 'anything', '.', '”']\n"
     ]
    }
   ],
   "source": [
    "print(\"{} data are split into {} train and {} test set\".format(\n",
    "    len(tokenized_data), len(train_data), len(test_data)))\n",
    "\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem you can bump into when using n-grams for predicting the upcoming word is what happens if you input a word that you have never seen in the dataset. These are called unknown words, or <b>out of vocabulary (OOV)</b> words. The percentage of unknown words in the test set is called the <b> OOV </b> rate. In such a case, you will not be able to predict the upcoming word because you do not have any counts for that specific word. To mitigate this problem, you would look at all the words in the training set, and only keep a list of the most frequent words. This will be your <b> closed vocabulary </b>. All the other words that are not part of your closed vocabulary will be changed to the word 'unk'. You will now create a function that takes in a text document, n, which is the number of most frequent words used that you want to keep, returns the document containing only the word closed vocabulary and the word unk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed: count_words ###\n",
    "def count_words(tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Count the number of word appearence in the tokenized sentences\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "    \n",
    "    Returns:\n",
    "        dict that maps word (str) to the frequency (int)\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 1,\n",
       " 'is': 1,\n",
       " 'blue': 1,\n",
       " '.': 3,\n",
       " 'leaves': 1,\n",
       " 'are': 2,\n",
       " 'green': 1,\n",
       " 'roses': 1,\n",
       " 'red': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "count_words(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected answer (order may differ):\n",
    "\n",
    "```\n",
    "{'sky': 1,\n",
    " 'is': 1,\n",
    " 'blue': 1,\n",
    " '.': 3,\n",
    " 'leaves': 1,\n",
    " 'are': 2,\n",
    " 'green': 1,\n",
    " 'roses': 1,\n",
    " 'red': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the word counts computed above, we will now find the N most frequence words.\n",
    "The implementation requires a bit of care, since dictionary type is essentially "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed: get_words_with_nplus_frequency ###\n",
    "def get_words_with_nplus_frequency(tokenized_sentences, minimum_freq):\n",
    "    \"\"\"\n",
    "    Find the words that appear N times or more\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of sentences\n",
    "        minimum_freq: Minimum frequency, i.e. N\n",
    "    \n",
    "    Returns:\n",
    "        List of words that appear N times or more\n",
    "    \"\"\"\n",
    "    filtered_words = []\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    \n",
    "    for word in word_counts:\n",
    "        if word_counts[word] >= minimum_freq:\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'are']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
    "                       ['leaves', 'are', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "get_words_with_nplus_frequency(tokenized_sentences, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected answer:\n",
    "\n",
    "```\n",
    "['.', 'are']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that appear n times or more are treated as our \"vocabulary\" and all other words are regarded as unknown.\n",
    "Hence, we will replace words not in our vocabulary by a marker \"<unk\\>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed: replace_oov_words_by_unk ###\n",
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_marker=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary by unknown marker\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "        vocabulary: List of strings that we will use\n",
    "        unknown_marker: A string representing unknown (out-of-vocabulary) words\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "    vocabulary = set(vocabulary)  # this makes each search faster\n",
    "    replaced_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in vocabulary:\n",
    "                replaced_sentence.append(word)\n",
    "            else:\n",
    "                replaced_sentence.append(\"<unk>\")\n",
    "        \n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dogs', '<unk>'], ['<unk>', 'sleep']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
    "vocabulary = [\"dogs\", \"sleep\"]\n",
    "replace_oov_words_by_unk(tokenized_sentences, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected answer:\n",
    "\n",
    "```\n",
    "['dogs', '<unk>'], ['<unk>', 'sleep']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to process our data combining the functions defined above.\n",
    "Specifically, we will\n",
    "\n",
    "1. Find tokens that appear at least N times in the training data.\n",
    "1. Replace tokens that appear less than N times by \"<unk\\>\" both for training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed: preprocess_data ###\n",
    "def preprocess_data(train_data, test_data, minimum_freq):\n",
    "    \"\"\"\n",
    "    Preprocess data, i.e.,\n",
    "        - Find tokens that appear at least N times in the training data.\n",
    "        - Replace tokens that appear less than N times by \"<unk>\" both for training and test data.        \n",
    "    Args:\n",
    "        train_data, test_data: List of lists of strings.\n",
    "        minimum_freq: Minimum frequency. Words that appear less than this times will be \n",
    "                      treated as unknown.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of\n",
    "        - training data with low frequent words replaced by \"<unk>\"\n",
    "        - test data with low frequent words replaced by \"<unk>\"\n",
    "        - vocabulary of words that appear n times or more in the training data\n",
    "    \"\"\"\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, minimum_freq)\n",
    "\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary)\n",
    "    \n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']],\n",
       " [['<unk>', 'are', '<unk>', '.']],\n",
       " ['sky', 'is', 'blue', '.', 'leaves', 'are', 'green'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "x = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green']]\n",
    "y = [['roses', 'are', 'red', '.']]\n",
    "\n",
    "preprocess_data(x, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected outcome:\n",
    "\n",
    "```\n",
    "([['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']],\n",
    " [['<unk>', 'are', '<unk>', '.']],\n",
    " ['sky', 'is', 'blue', '.', 'leaves', 'are', 'green'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the cell below to complete the preprocessing both for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(\n",
    "    train_data, test_data, minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First preprocessed training sample:\n",
      "['Adam', 'Sandler', 'is', 'not', 'being', 'funny', 'on', '#', '<unk>', '.', 'Make', 'it', 'stop']\n",
      "First preprocessed test sample:\n",
      "['“', ':', 'Dwayne', 'Wade', 'accused', 'Pacers', 'of', 'excessive', 'celebration', '.', 'the', 'Heat', 'would', 'NEVER', 'celebrate', 'before', 'they', 'won', 'anything', '.', '”']\n",
      "First 10 vocabulary:\n",
      "['Adam', 'Sandler', 'is', 'not', 'being', 'funny', 'on', '#', '.', 'Make']\n",
      "Size of vocabulary: 188567\n"
     ]
    }
   ],
   "source": [
    "print(\"First preprocessed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "\n",
    "print(\"First preprocessed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(vocabulary[0:10])\n",
    "\n",
    "print(\"Size of vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done with the preprocessing section of the assignment.\n",
    "Objects `train_data_processed`, `test_data_processed`, are `vocabulary` to be used in the rest of the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Develop n-gram based language models\n",
    "\n",
    "In this section, we will develop language models using the data prepared in the previous section.\n",
    "Our modelling strategy is **n-grams**; we assume the probability of the next word depends only on the previous n-gram (i.e., $n$ words). \n",
    "Mathematically, we can write the conditional probability for the t-th word as:\n",
    "\n",
    "$$ P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}$$\n",
    "\n",
    "where $w_t$ denotes the $t$-th word in a sentence.\n",
    "\n",
    "We will estimate this probability from the training data.\n",
    "A natural estimator for the probability is the fraction of counts: \n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
    "\n",
    "where $C(\\cdots)$ denotes the number of occurence of the given sequence. $\\hat{P}$ means the estimation of $P$. Notice that denominator of the equation (2) is the number of occurence of the previous $n$ words, and the numerator is the same sequence followed by the word $w_t$.\n",
    "\n",
    "As covered in the lecture, we will later modify the equation (2) by adding k-smoothing, which avoid errors in zero count cases.\n",
    "\n",
    "The equation (2) tells us that to estimate probabilities based on n-grams, we need the counts of n-grams (for denominator) and (n+1)-grams (for numerator).\n",
    "So, in the next cell, you will implement a function that computes the counts of n-grams for an arbitrary $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the counts for n-grams, we prepend $n-1$ starting markers \"<s\\>\" to indicate the beginning of the sentence.  For example, in the bi-gram model, a sequence \"<s\\><s\\>\" should predict the first word of a sentence.\n",
    "We also append a ending marker \"<e\\>\" so that the model can predict when to finish a sentence.\n",
    "\n",
    "Technical note: In this implementation, we will store the counts as a dictionary that maps a tuple of n words to the number of occurence.  The reason why we use tuple instead of list is because a list in Python is mutable object, and hence it is not intended to be used as a key to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed: count_n_grams ###\n",
    "def count_n_grams(data, n):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the data\n",
    "    \n",
    "    Args:\n",
    "        data: List of lists of words\n",
    "        n: number of words in a sequence\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    n_grams = {}\n",
    "    for sentence in data:\n",
    "        # prepend <s> n times and append <s>\n",
    "        sentence = [\"<s>\"]*n + sentence + [\"<e>\"]\n",
    "        # convert list to tuple for using sequence as dictionary keys\n",
    "        sentence = tuple(sentence)\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            if (sentence[i:i + n]) in n_grams:\n",
    "                n_grams[(sentence[i:i + n])] += 1\n",
    "            else:\n",
    "                n_grams[(sentence[i:i + n])] = 1\n",
    "        \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "# Outcome does not match expected outcome\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected outcome:\n",
    "\n",
    "```\n",
    "Uni-gram:\n",
    "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
    "Bi-gram:\n",
    "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we estimate the probability using the n-gram counts.\n",
    "We show the equation (2) again here:\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
    "\n",
    "While this is a natural estimator, it has limitation in dealing with zero count cases.\n",
    "Suppose we encounter an n-gram that did not occur in the training data.  Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).\n",
    "A way to avoid such case is to add k-smoothing.  In effect, it adds a positive constant $k$ to each numerator counts in the equation (2):\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "where $|V|$ is the size of vocabulary.\n",
    "\n",
    "For unseen n-grams, the equation (3) becomes $1/|V|$, i.e. picks all words with the same probability.\n",
    "\n",
    "In the next cell, you will define a function that computes the probability estimate (3) from n-gram counts and a constant $k$.\n",
    "\n",
    "Hint: In the imprementation, we use pre-computed n-gram and (n+1)-gram counts for calculating the equation (3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed: estimate_probabilityy ###\n",
    "def estimate_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        word: next word\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of words in the vocabulary\n",
    "        k: positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected outcome:\n",
    "\n",
    "```\n",
    "0.3333333333333333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined below loops over all words in vocabulary to calculate probabilities for all possible words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary mapping from next words to the probability.\n",
    "    \"\"\"\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is not needed since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected outcome:\n",
    "\n",
    "```\n",
    "{'is': 0.09090909090909091,\n",
    " 'like': 0.09090909090909091,\n",
    " 'dog': 0.09090909090909091,\n",
    " 'cat': 0.2727272727272727,\n",
    " 'i': 0.09090909090909091,\n",
    " 'a': 0.09090909090909091,\n",
    " 'this': 0.09090909090909091,\n",
    " '<e>': 0.09090909090909091,\n",
    " '<unk>': 0.09090909090909091}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected outcome:\n",
    "\n",
    "```\n",
    "{'is': 0.09090909090909091,\n",
    " 'like': 0.09090909090909091,\n",
    " 'dog': 0.09090909090909091,\n",
    " 'cat': 0.09090909090909091,\n",
    " 'i': 0.18181818181818182,\n",
    " 'a': 0.09090909090909091,\n",
    " 'this': 0.18181818181818182,\n",
    " '<e>': 0.09090909090909091,\n",
    " '<unk>': 0.09090909090909091}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count and probability matrices\n",
    "\n",
    "As we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the nex words.  Yet, it might be more intuitive to present them as count or probability matrices that we covered in the lecture.\n",
    "The functions defined in the next cells return count or probability matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is omitted since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    \n",
    "    # obtain unique n-grams\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    # mapping from n-gram to row\n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "    # mapping from next word to column\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "display(make_count_matrix(bigram_counts, unique_words))\n",
    "\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))\n",
    "\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that we obtain the same results as for the `estimate_probabilities` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Perplexity\n",
    "\n",
    "In this section, you will generate the perplexity score to evaluate your model on the test set. You will also use back-off when needed. Perplexity is used as an evaluation metric of your language model. If you were to calculate the  the perplexity score of your test set, on a bigram model, you compute the following formula: \n",
    "$$ PP(W) =\\sqrt[N]{ \\prod_{t=1}^N \\frac{1}{P(w_t | w_{t-1})} } \\tag{4}$$\n",
    "\n",
    "where $N$ is the length of the sentence.\n",
    "\n",
    "The higher our probabilities are the lower the perplexity will be. The more our n-grams tell us about the sentence the lower our perplexity score will be. \n",
    "\n",
    "</b> \n",
    "<b>Instructions:</b> Compute the perplexity score given an N-gram count matrix and a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed: calculate_perplexity\n",
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of strings\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of unique words in the vocabulary\n",
    "        k: Positive smoothing constant\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity score\n",
    "    \"\"\"\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    sentence = tuple(sentence)\n",
    "    N = len(sentence)\n",
    "    p = 1.0\n",
    "        \n",
    "    for i in range(N - n):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    perplexity = (1/p)**(1/N)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code\n",
    "\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity_train1 = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(\"Perplexity for first train sample:\", perplexity_train1)\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = calculate_perplexity(test_sentence,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(\"Perplexity for test sample:\", perplexity_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output: \n",
    "\n",
    "```\n",
    "2.8039657955522013\n",
    "3.965406456500188\n",
    "```\n",
    "\n",
    "<b> Note: </b> If your sentence is really long, then you take the sum of the log of the probabilities to avoid underflow errors, instead of multiplying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Develop auto-complete system\n",
    "\n",
    "In this section, we will combine the language models developed so far to finally implement an auto-complete system. \n",
    "In this implementation, we compute probabilities for all possible next word and suggest the most likely one.\n",
    "This function also take an optional argument `start_with`, which specifies the first a few letters of the next words.\n",
    "\n",
    "Hint: `estimate_probabilities` returns a mapping from word to the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed: suggest_a_word\n",
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    \n",
    "    Args:\n",
    "        previous_tokens: The sentence you input where each token is a word. Must have length > n \n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "        start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of \n",
    "          - string of the most likely next word\n",
    "          - corresponding probability\n",
    "    \"\"\"\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code\n",
    "suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected outcomes:\n",
    "\n",
    "```\n",
    "('a', 0.2727272727272727)\n",
    "('cat', 0.09090909090909091)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined below loop over varioud n-gram models to get multiple suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show time!\n",
    "\n",
    "Conguraturations!  You have developed all building blocks for implementing your own auto-complete systems.\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"i\", \"am\", \"to\"]\n",
    "get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n",
    "get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
    "get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
